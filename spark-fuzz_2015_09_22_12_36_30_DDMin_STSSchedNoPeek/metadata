{
  "additional_metadata": null,
  "argv": [
    "./../../../../.././interposition/src/main/python/setup.py",
    "-n",
    "spark-fuzz_2015_09_22_12_36_30_DDMin_STSSchedNoPeek"
  ],
  "cwd": "/Users/cs/Research/UCB/code/sts2-applications",
  "host": {
    "cpu_info": "",
    "free": "",
    "name": "yossarian",
    "num_cores": "0",
    "uptime": "12:36  up 5 days, 15:28, 8 users, load averages: 2.00 2.25 2.18"
  },
  "modules": {
    "sts2": {
      "branch": "spark-3150",
      "commit": "1d80914e6fd44ca2aa0f485534bc3084ba20cfce",
      "diff": "diff --git a/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala b/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala\nindex 4316ef1..20df888 100644\n--- a/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala\n+++ b/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala\n@@ -55,32 +55,39 @@ class SparkMessageFingerprinter extends MessageFingerprinter {\n     }\n \n     val str = msg match {\n-      case BlockManagerMessages.RegisterBlockManager(_,_,_) =>\n+      case BlockManagerMessages.RegisterBlockManager(manager_id,maxmem,sender) =>\n         \"RegisterBlockManager\"\n-      case BlockManagerMessages.HeartBeat(_) =>\n+      case BlockManagerMessages.HeartBeat(manager_id) =>\n         \"HeartBeat\"\n-      case JobSubmitted(jobId,_,_,_,_,_,_,_) =>\n+      case JobSubmitted(jobId,finalRDD,func,partitions,allowLocal,callSite,listener,props) =>\n         (\"JobSubmitted\", jobId).toString\n-      case BlockManagerMessages.GetLocationsMultipleBlockIds(_) =>\n-        \"GetLocationsMultipleBlockIds\"\n-      case BeginEvent(task,_) =>\n+      case BlockManagerMessages.UpdateBlockInfo(blockManagerId, blockId, storageLevel, memSize, diskSize, tachyonSize) =>\n+        (\"UpdateBlockInfo\", blockId, storageLevel).toString\n+      //case BlockManagerMessages.GetLocationsMultipleBlockIds(blockIds) =>\n+      //  // TODO(cs): how are block ids named? rdd_?_?\n+      //  \"GetLocationsMultipleBlockIds\"\n+      case BeginEvent(task,taskInfo) =>\n         (\"BeginEvent\", task).toString\n-      case CompletionEvent(task, reason, _, _, _, _) =>\n+      case CompletionEvent(task, reason, result, accumUpdates, taskInfo, taskMetrics) =>\n         (\"CompletionEvent\", task, reason).toString\n-      case org.apache.spark.scheduler.local.StatusUpdate(id, state, _) =>\n+      case org.apache.spark.scheduler.local.StatusUpdate(id, state, data) =>\n         (\"StatusUpdate\", id, state).toString\n-      case CoarseGrainedClusterMessages.StatusUpdate(execId, tid, state, _) =>\n+      case CoarseGrainedClusterMessages.StatusUpdate(execId, tid, state, data) =>\n         (\"StatusUpdate\", execId, tid, state).toString\n-      case DeployMessages.RegisteredApplication(_, _) =>\n+      case DeployMessages.RegisteredApplication(appId, masterUrl) =>\n         (\"RegisteredApplication\").toString\n-      case DeployMessages.ExecutorStateChanged(_, id, state, _, _) =>\n+      case DeployMessages.ExecutorStateChanged(appId, id, state, message, exitStatus) =>\n         (\"ExecutorStateChanged\", id, state).toString\n       case DeployMessages.RegisterWorker(id, host, port, cores, memory, webUiPort, publicAddress) =>\n         (\"RegisterWorker\", id).toString\n-      case DeployMessages.RegisteredWorker(_, _) =>\n+      case DeployMessages.RegisteredWorker(masterUrl, masterWebUrl) =>\n         (\"RegisteredWorker\").toString\n-      case CoarseGrainedClusterMessages.LaunchTask(_) =>\n+      case CoarseGrainedClusterMessages.LaunchTask(data) =>\n         (\"LaunchTask\").toString\n+      //case s: Seq[Seq[BlockManagerId]] =>\n+      //  s\n+      //case s: Seq[BlockManagerId] =>\n+      //  s\n       case m =>\n         \"\"\n     }\n@@ -437,6 +444,24 @@ object STSSparkPi {\n           val mcs_dir = serializer.serializeMCS(dir, mcs, stats1,\n              verified_mcs, violation, false)\n           println(\"MCS DIR: \" + mcs_dir)\n+\n+          // Hack\n+          val msgSerializer = new BasicMessageSerializer\n+          val msgDeserializer = new BasicMessageDeserializer(loader=Thread.currentThread.getContextClassLoader)\n+\n+          def shouldRerunDDMin(externals: Seq[ExternalEvent]) =\n+            false // XXX\n+\n+          RunnerUtils.runTheGamut(dir, mcs_dir, schedulerConfig, msgSerializer,\n+            msgDeserializer, shouldRerunDDMin=shouldRerunDDMin,\n+            populateActors=false,\n+            atomIndices=Some(Seq((1,8),(3,9),(21,22))),\n+            loader=Thread.currentThread.getContextClassLoader,\n+            initializationRoutine=Some(runAndCleanup),\n+            preTest=Some(preTest), postTest=Some(postTest),\n+            clusteringStrategy=ClusteringStrategy.SingletonClusterizer,\n+            fungClocksScheduler=TestScheduler.STSSched)\n+            //paranoid=false)\n         case None =>\n           println(\"Job finished successfully...\")\n       }\n@@ -444,9 +469,11 @@ object STSSparkPi {\n \n     if (!fuzz) {\n       val dir =\n-      \"/Users/cs/Research/UCB/code/sts2-applications/experiments/spark-fuzz_2015_09_15_13_08_54\"\n+      //\"/Users/cs/Research/UCB/code/sts2-applications/experiments/spark-fuzz_2015_09_15_13_08_54\"\n+      \"/Users/cs/Research/UCB/code/sts2-applications/experiments/spark-fuzz_2015_09_22_12_03_12\"\n       val mcs_dir =\n-      \"/Users/cs/Research/UCB/code/sts2-applications/experiments/spark-fuzz_2015_09_15_13_08_54_DDMin_STSSchedNoPeek\"\n+      //\"/Users/cs/Research/UCB/code/sts2-applications/experiments/spark-fuzz_2015_09_15_13_08_54_DDMin_STSSchedNoPeek\"\n+      \"/Users/cs/Research/UCB/code/sts2-applications/experiments/spark-fuzz_2015_09_22_12_03_12_DDMin_STSSchedNoPeek\"\n \n       val msgSerializer = new BasicMessageSerializer\n       val msgDeserializer = new BasicMessageDeserializer(loader=Thread.currentThread.getContextClassLoader)\n@@ -460,7 +487,10 @@ object STSSparkPi {\n         atomIndices=Some(Seq((1,8),(3,9),(21,22))),\n         loader=Thread.currentThread.getContextClassLoader,\n         initializationRoutine=Some(runAndCleanup),\n-        preTest=Some(preTest), postTest=Some(postTest))\n+        preTest=Some(preTest), postTest=Some(postTest),\n+        clusteringStrategy=ClusteringStrategy.SingletonClusterizer,\n+        fungClocksScheduler=TestScheduler.STSSched)\n+        //paranoid=false)\n     }\n   }",
      "status": "On branch spark-3150\nYour branch is up-to-date with 'origin/spark-3150'.\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\n\n\tmodified:   src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")"
    }
  },
  "sys": {
    "lsb_release": "",
    "uname": "Darwin yossarian 14.5.0 Darwin Kernel Version 14.5.0: Wed Jul 29 02:26:53 PDT 2015; root:xnu-2782.40.9~1/RELEASE_X86_64 x86_64"
  },
  "timestamp": "2015_09_22_12_36_30",
  "user": "cs"
}
