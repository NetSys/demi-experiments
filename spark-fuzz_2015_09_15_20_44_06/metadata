{
  "additional_metadata": null,
  "argv": [
    "./../../../../.././interposition/src/main/python/setup.py",
    "-t",
    "-n",
    "spark-fuzz"
  ],
  "cwd": "/Users/cs/Research/UCB/code/sts2-applications",
  "host": {
    "cpu_info": "",
    "free": "",
    "name": "yossarian",
    "num_cores": "0",
    "uptime": "20:44  up 22 days, 18:40, 8 users, load averages: 2.02 1.77 1.55"
  },
  "modules": {
    "sts2": {
      "branch": "spark-9256",
      "commit": "b45d1aba52bb86c8be9b17034e9ffbe774e81593",
      "diff": "diff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/Client.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/Client.scala\nindex 17c507a..27d3b0f 100644\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/Client.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/Client.scala\n@@ -95,7 +95,7 @@ private class ClientActor(driverArgs: ClientArguments, conf: SparkConf) extends\n     statusResponse.found match {\n       case false =>\n         println(s\"ERROR: Cluster master did not recognize $driverId\")\n-        System.exit(-1)\n+        //System.exit(-1)\n       case true =>\n         println(s\"State of $driverId is ${statusResponse.state.get}\")\n         // Worker node, if present\n@@ -108,9 +108,9 @@ private class ClientActor(driverArgs: ClientArguments, conf: SparkConf) extends\n         statusResponse.exception.map { e =>\n           println(s\"Exception from cluster was: $e\")\n           e.printStackTrace()\n-          System.exit(-1)\n+          //System.exit(-1)\n         }\n-        System.exit(0)\n+        //System.exit(0)\n     }\n   }\n \n@@ -118,20 +118,20 @@ private class ClientActor(driverArgs: ClientArguments, conf: SparkConf) extends\n \n     case SubmitDriverResponse(success, driverId, message) =>\n       println(message)\n-      if (success) pollAndReportStatus(driverId.get) else System.exit(-1)\n+      if (success) pollAndReportStatus(driverId.get) else null //System.exit(-1)\n \n     case KillDriverResponse(driverId, success, message) =>\n       println(message)\n-      if (success) pollAndReportStatus(driverId) else System.exit(-1)\n+      if (success) pollAndReportStatus(driverId) else null //System.exit(-1)\n \n     case DisassociatedEvent(_, remoteAddress, _) =>\n       println(s\"Error connecting to master ${driverArgs.master} ($remoteAddress), exiting.\")\n-      System.exit(-1)\n+      //System.exit(-1)\n \n     case AssociationErrorEvent(cause, _, remoteAddress, _) =>\n       println(s\"Error connecting to master ${driverArgs.master} ($remoteAddress), exiting.\")\n       println(s\"Cause was: $cause\")\n-      System.exit(-1)\n+      //System.exit(-1)\n   }\n }\n \ndiff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala\nindex 39150de..bd15e58 100644\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala\n@@ -109,7 +109,7 @@ private[spark] class ClientArguments(args: Array[String]) {\n       |   -v, --verbose                  Print more debugging output\n      \"\"\".stripMargin\n     System.err.println(usage)\n-    System.exit(exitCode)\n+    //System.exit(exitCode)\n   }\n }\n \ndiff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala\nindex 0d6751f..c82df09 100644\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala\n@@ -63,7 +63,7 @@ object PythonRunner {\n \n     new RedirectThread(process.getInputStream, System.out, \"redirect output\").start()\n \n-    System.exit(process.waitFor())\n+    //System.exit(process.waitFor())\n   }\n \n   /**\ndiff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala\nindex 0c29156..68e7c7e 100644\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala\n@@ -55,7 +55,7 @@ object SparkSubmit {\n   private val PYSPARK_SHELL = \"pyspark-shell\"\n \n   // Exposed for testing\n-  private[spark] var exitFn: () => Unit = () => System.exit(-1)\n+  private[spark] var exitFn: () => Unit = () => null //System.exit(-1)\n   private[spark] var printStream: PrintStream = System.err\n   private[spark] def printWarning(str: String) = printStream.println(\"Warning: \" + str)\n   private[spark] def printErrorAndExit(str: String) = {\ndiff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/client/TestClient.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/client/TestClient.scala\nindex b8ffa9a..db68379 100644\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/client/TestClient.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/client/TestClient.scala\n@@ -30,12 +30,12 @@ private[spark] object TestClient {\n \n     def disconnected() {\n       logInfo(\"Disconnected from master\")\n-      System.exit(0)\n+      //System.exit(0)\n     }\n \n     def dead(reason: String) {\n       logInfo(\"Application died with error: \" + reason)\n-      System.exit(0)\n+      //System.exit(0)\n     }\n \n     def executorAdded(id: String, workerId: String, hostPort: String, cores: Int, memory: Int) {}\ndiff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala\nindex be9361b..cee4856 100644\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala\n@@ -71,7 +71,7 @@ private[spark] class HistoryServerArguments(conf: SparkConf, args: Array[String]\n       |  spark.history.fs.updateInterval    How often to reload log data from storage (in seconds,\n       |                                     default 10)\n       |\"\"\".stripMargin)\n-    System.exit(exitCode)\n+    //System.exit(exitCode)\n   }\n \n }\ndiff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala\nindex a87781f..2d043cc 100644\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala\n@@ -84,6 +84,6 @@ private[spark] class MasterArguments(args: Array[String], conf: SparkConf) {\n       \"  -h HOST, --host HOST   Hostname to listen on\\n\" +\n       \"  -p PORT, --port PORT   Port to listen on (default: 7077)\\n\" +\n       \"  --webui-port PORT      Port for web UI (default: 8080)\")\n-    System.exit(exitCode)\n+    //System.exit(exitCode)\n   }\n }\ndiff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala\nindex ea0d4d6..50369c9 100644\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala\n@@ -44,7 +44,7 @@ object DriverWrapper {\n \n       case _ =>\n         System.err.println(\"Usage: DriverWrapper <workerUrl> <driverMainClass> [options]\")\n-        System.exit(-1)\n+        //System.exit(-1)\n     }\n   }\n }\ndiff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala\nindex a3f14de..64e018d 100755\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala\n@@ -199,7 +199,7 @@ private[spark] class Worker(\n             registrationRetryTimer.foreach(_.cancel())\n           } else if (retries >= REGISTRATION_RETRIES) {\n             logError(\"All masters are unresponsive! Giving up.\")\n-            System.exit(1)\n+            //System.exit(1)\n           } else {\n             tryRegisterAllMasters()\n           }\ndiff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala\nindex dc51581..18be109 100644\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala\n@@ -119,7 +119,7 @@ private[spark] class WorkerArguments(args: Array[String]) {\n       \"  -h HOST, --host HOST     Hostname to listen on\\n\" +\n       \"  -p PORT, --port PORT     Port to listen on (default: random)\\n\" +\n       \"  --webui-port PORT        Port for web UI (default: 8081)\")\n-    System.exit(exitCode)\n+    //System.exit(exitCode)\n   }\n \n   def inferDefaultCores(): Int = {\ndiff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala\nindex 2f1cb2d..23361cd 100644\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala\n@@ -47,7 +47,7 @@ private[spark] class WorkerWatcher(workerUrl: String) extends Actor\n   private val expectedHostPort = AddressFromURIString(workerUrl).hostPort\n   private def isWorker(address: Address) = address.hostPort == expectedHostPort\n \n-  def exitNonZero() = if (isTesting) isShutDown = true else System.exit(-1)\n+  def exitNonZero() = if (isTesting) isShutDown = true else null //System.exit(-1)\n \n   override def receive = {\n     //case AssociatedEvent(localAddress, remoteAddress, inbound) if isWorker(remoteAddress) =>\ndiff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala\nindex 7f52262..5e48083 100644\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala\n@@ -196,7 +196,7 @@ private[spark] object CoarseGrainedExecutorBackend extends Logging {\n           // Worker url is used in spark standalone mode to enforce fate-sharing with worker\n           \"Usage: CoarseGrainedExecutorBackend <driverUrl> <executorId> <hostname> \" +\n           \"<cores> [<workerUrl>]\")\n-        System.exit(1)\n+        //System.exit(1)\n       case 4 =>\n         run(args(0), args(1), args(2), args(3).toInt, None)\n       case x if x > 4 =>\ndiff --git a/src/main/scala/spark/core/src/main/scala/org/apache/spark/network/netty/ShuffleCopier.scala b/src/main/scala/spark/core/src/main/scala/org/apache/spark/network/netty/ShuffleCopier.scala\nindex e7b2855..5bc9b10 100644\n--- a/src/main/scala/spark/core/src/main/scala/org/apache/spark/network/netty/ShuffleCopier.scala\n+++ b/src/main/scala/spark/core/src/main/scala/org/apache/spark/network/netty/ShuffleCopier.scala\n@@ -95,7 +95,7 @@ private[spark] object ShuffleCopier extends Logging {\n   def main(args: Array[String]) {\n     if (args.length < 3) {\n       System.err.println(\"Usage: ShuffleCopier <host> <port> <shuffle_block_id> <threads>\")\n-      System.exit(1)\n+      //System.exit(1)\n     }\n     val host = args(0)\n     val port = args(1).toInt\n@@ -113,6 +113,6 @@ private[spark] object ShuffleCopier extends Logging {\n     }).asJava\n     copiers.invokeAll(tasks)\n     copiers.shutdown()\n-    System.exit(0)\n+    //System.exit(0)\n   }\n }",
      "status": "On branch spark-9256\nYour branch is up-to-date with 'origin/spark-9256'.\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\n\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/Client.scala\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/client/TestClient.scala\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala\n\tmodified:   src/main/scala/spark/core/src/main/scala/org/apache/spark/network/netty/ShuffleCopier.scala\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\n\tinterposition/src/main/python/lifecycle.pyc\n\tinterposition/src/main/python/util.pyc\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")"
    }
  },
  "sys": {
    "lsb_release": "",
    "uname": "Darwin yossarian 14.5.0 Darwin Kernel Version 14.5.0: Wed Jul 29 02:26:53 PDT 2015; root:xnu-2782.40.9~1/RELEASE_X86_64 x86_64"
  },
  "timestamp": "2015_09_15_20_44_06",
  "user": "cs"
}
