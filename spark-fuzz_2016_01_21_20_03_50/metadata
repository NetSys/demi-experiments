{
  "additional_metadata": null,
  "argv": [
    "./../../../../.././interposition/src/main/python/setup.py",
    "-t",
    "-n",
    "spark-fuzz"
  ],
  "cwd": "/Users/cs/Research/UCB/code/demi-applications",
  "host": {
    "cpu_info": "",
    "free": "",
    "name": "yossarian",
    "num_cores": "0",
    "uptime": "20:03  up  9:01, 7 users, load averages: 1.70 2.57 2.23"
  },
  "modules": {
    "sts2": {
      "branch": "spark-9256",
      "commit": "e80a761d9ebdb5fc68fdb96c47f8296d498b6dbb",
      "diff": "diff --git a/interposition/src/main/scala/verification/schedulers/InteractiveScheduler.scala b/interposition/src/main/scala/verification/schedulers/InteractiveScheduler.scala\nindex 26d9f03..889b3c9 100644\n--- a/interposition/src/main/scala/verification/schedulers/InteractiveScheduler.scala\n+++ b/interposition/src/main/scala/verification/schedulers/InteractiveScheduler.scala\n@@ -239,10 +239,42 @@ class InteractiveScheduler(val schedulerConfig: SchedulerConfig)\n   var externals : Seq[ExternalEvent] = null\n \n   def run(_externals:Seq[ExternalEvent]): Tuple2[EventTrace,Option[ViolationFingerprint]] = {\n+    /*\n+    preTestCallback()\n+\n+    // Kick off the system's initialization routine\n+    var initThread : Thread = null\n+    initializationRoutine match {\n+      case Some(f) =>\n+        logger.trace(\"Running initializationRoutine...\")\n+        initThread = new Thread(\n+          new Runnable { def run() = { f() } },\n+          \"initializationRoutine\")\n+        initThread.start\n+      case None =>\n+    }\n+    */\n+\n     externals = _externals\n     event_orchestrator.events.setOriginalExternalEvents(_externals)\n     Instrumenter().scheduler = this\n+\n+    /*\n+     wait for EndExternalAtomicBlocks?\n+     */\n+\n     return (execute_trace(_externals), violation)\n+\n+    /*\n+    postTestCallback()\n+\n+    // Wait until the initialization thread is done. Assumes that it\n+    // terminates!\n+    if (initThread != null) {\n+      logger.debug(\"Joining on initialization thread \" + Thread.currentThread.getName)\n+      initThread.join\n+    }\n+    */\n   }\n \n   // Record a mapping from actor names to actor refs\ndiff --git a/invocation.txt b/invocation.txt\nindex d082861..29187f9 100644\n--- a/invocation.txt\n+++ b/invocation.txt\n@@ -4,8 +4,7 @@\n /usr/bin/java -Xmx2500M -XX:MaxPermSize=256m -XX:+CMSClassUnloadingEnabled -jar /usr/local/Cellar/sbt/0.13.0/libexec/sbt-launch.jar \"project examples\" assembly\n \n # Then run\n-sbt \"project core\" \"run --master local-cluster[2,1,512] --class org.apache.spark.examples.STSSparkPi /Users/cs/Research/UCB/code/sts2-applications/src/main/scala/spark/examples/target/scala-2.10/spark-examples-1.1.0-SNAPSHOT-hadoop1.0.4.jar\"\n-\n+sbt \"project core\" \"run --master local-cluster[2,1,512] --class org.apache.spark.examples.STSSparkPi /Users/cs/Research/UCB/code/demi-applications/src/main/scala/spark/examples/target/scala-2.10/spark-examples-1.1.0-SNAPSHOT-hadoop1.0.4.jar\"\n \n -- Notes ---\n \ndiff --git a/project/SparkBuild.scala b/project/SparkBuild.scala\nindex 2b77e83..7d1d569 100644\n--- a/project/SparkBuild.scala\n+++ b/project/SparkBuild.scala\n@@ -73,7 +73,7 @@ object SparkBuild extends Build {\n     // For 2.10.*:\n     libraryDependencies += \"org.scala-lang\" % \"scala-swing\" % scalaVersion.value,\n     libraryDependencies += \"com.typesafe.akka\" %% \"akka-actor\" % \"2.2.3\",\n-    libraryDependencies += \"com.typesafe.akka\" %% \"akka-cluster\" % \"2.2.3\"\n+    libraryDependencies += \"com.typesafe.akka\" %% \"akka-cluster\" % \"2.2.3\",\n     libraryDependencies += \"org.scala-lang.modules\" % \"scala-jline\" % \"2.12.1\"\n \n     // -- /STS deps --\ndiff --git a/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/DuplicateFileCrash.scala b/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/DuplicateFileCrash.scala\nindex 6afaf02..0dc9915 100644\n--- a/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/DuplicateFileCrash.scala\n+++ b/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/DuplicateFileCrash.scala\n@@ -11,7 +11,7 @@ case class DuplicateFileCrash() extends ViolationFingerprint {\n     }\n     return true\n   }\n-  def affectedNodes(): Seq[String] = Seq.empty\n+  def affectedNodes(): Seq[String] = Seq(\"Master\")\n }\n \n object DuplicateFileCrash {\ndiff --git a/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala b/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala\nindex 7852483..bc04ddd 100644\n--- a/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala\n+++ b/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala\n@@ -29,6 +29,12 @@ import org.apache.spark.deploy.master.Master\n import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages\n import org.apache.spark.deploy._\n \n+import scalax.collection.mutable.Graph,\n+       scalax.collection.GraphEdge.DiEdge,\n+              scalax.collection.edge.LDiEdge\n+\n+\n+\n import akka.dispatch.verification._\n \n import org.slf4j.LoggerFactory\n@@ -37,6 +43,7 @@ import ch.qos.logback.classic.Logger\n \n import scala.collection.mutable.SynchronizedQueue\n import scala.collection.mutable.ListBuffer\n+import scala.collection.mutable.Queue\n \n import org.apache.spark.deploy.DeployMessages\n \n@@ -344,11 +351,11 @@ object STSSparkPi {\n       shouldShutdownActorSystem=false,\n       filterKnownAbsents=false,\n       ignoreTimers=false, // XXX\n-      invariant_check=Some(CrashUponRecovery.invariant))\n+      invariant_check=Some(DuplicateFileCrash.invariant))\n \n     val sched = new RandomScheduler(schedulerConfig,\n       invariant_check_interval=300, randomizationStrategy=new SrcDstFIFO)\n-    sched.setMaxMessages(100000)\n+    sched.setMaxMessages(10000)\n     Instrumenter().scheduler = sched\n \n     val (externals, atomicPairs) = getExternals()\n@@ -360,7 +367,7 @@ object STSSparkPi {\n       MyVariables.prematureStopSempahore.release()\n     }\n \n-    val fuzz = false\n+    val fuzz = true\n     if (fuzz) {\n       sched.nonBlockingExplore(externals, terminationCallback)\n \n@@ -379,19 +386,24 @@ object STSSparkPi {\n           println(\"Violation was found! Trying replay\")\n           val depGraph = sched.depTracker.getGraph\n           val initialTrace = sched.depTracker.getInitialTrace\n+          val filteredTrace = RunnerUtils.pruneConcurrentEvents(initialTrace,\n+            initTrace, violation, depGraph)\n \n           val sts = new STSScheduler(schedulerConfig, initTrace, false)\n           sts.setPreTestCallback(preTest)\n           sts.setPostTestCallback(postTest)\n \n-          val dag = new UnmodifiedEventDag(initTrace.original_externals flatMap {\n+          val truncatedExternals = initTrace.original_externals flatMap {\n             case WaitQuiescence() => None\n             case WaitCondition(_) => None\n             case e => Some(e)\n-          })\n+          }\n+          val dag = new UnmodifiedEventDag(truncatedExternals)\n           // Conjoin the HardKill and the subsequent recover\n           atomicPairs foreach {\n             case ((e1, e2)) =>\n+              if ((truncatedExternals contains e1) &&\n+                  (truncatedExternals contains e2))\n               dag.conjoinAtoms(e1, e2)\n           }\n \n@@ -423,7 +435,7 @@ object STSSparkPi {\n           val dir = serializer.record_experiment(\"spark-fuzz\",\n              initTrace, violation,\n              depGraph=Some(depGraph), initialTrace=Some(initialTrace),\n-             filteredTrace=None)\n+             filteredTrace=Some(filteredTrace))\n \n           val mcs_dir = serializer.serializeMCS(dir, mcs, stats1,\n              verified_mcs, violation, false)\n@@ -435,9 +447,9 @@ object STSSparkPi {\n \n     if (!fuzz) {\n       val dir =\n-      \"/Users/cs/Research/UCB/code/sts2-applications/experiments/spark-fuzz_2015_09_15_20_44_06\"\n+      \"/Users/cs/Research/UCB/code/demi-applications/experiments/spark-fuzz_2016_01_21_19_28_36\"\n       val mcs_dir =\n-      \"/Users/cs/Research/UCB/code/sts2-applications/experiments/spark-fuzz_2015_09_15_20_44_06_DDMin_STSSchedNoPeek\"\n+      \"/Users/cs/Research/UCB/code/demi-applications/experiments/spark-fuzz_2016_01_21_19_28_36_DDMin_STSSchedNoPeek\"\n \n       val msgSerializer = new BasicMessageSerializer\n       val msgDeserializer = new BasicMessageDeserializer(loader=Thread.currentThread.getContextClassLoader)\n@@ -445,10 +457,54 @@ object STSSparkPi {\n       def shouldRerunDDMin(externals: Seq[ExternalEvent]) =\n         false // XXX\n \n+        /*\n+      val (trace, violation, depGraph: Option[Graph[Unique, DiEdge]]) = RunnerUtils.deserializeExperiment(dir,\n+        msgDeserializer,\n+        //traceFile=ExperimentSerializer.,\n+        loader=Thread.currentThread.getContextClassLoader)\n+\n+      val deserializer = new ExperimentDeserializer(dir,\n+        loader=Thread.currentThread.getContextClassLoader)\n+\n+      val initialTrace = deserializer.get_initial_trace\n+\n+      var filtered = new Queue[Unique]\n+      val provenenceTracker = new ProvenanceTracker(initialTrace.get,\n+        depGraph.get)\n+      val origDeliveries = RunnerUtils.countMsgEvents(trace.filterCheckpointMessages.filterFailureDetectorMessages)\n+      filtered = provenenceTracker.pruneConcurrentEvents(violation)\n+      val numberFiltered = origDeliveries - RunnerUtils.countMsgEvents(filtered.map(u => u.event))\n+      // TODO(cs): track this number somewhere. Or reconstruct it from\n+      // initialTrace/filtered.\n+      println(\"Pruned \" + numberFiltered + \"/\" + origDeliveries + \" concurrent deliveries\")\n+\n+      // val tAsArray : Array[Unique] = filtered.toArray\n+      // val traceBuf = JavaSerialization.serialize(tAsArray)\n+      // JavaSerialization.writeToFile(dir + ExperimentSerializer.filteredTrace, traceBuf)\n+      // JavaSerialization.writeToFile(mcs_dir + ExperimentSerializer.filteredTrace, traceBuf)\n+\n+      println(s\"filteredTrace ${filtered.size}\")\n+\n+      // val (mcs, trace, violation, actors, stats) = RunnerUtils.deserializeMCS(mcs_dir,\n+      //    msgDeserializer,\n+      //    traceFile=ExperimentSerializer.minimizedInternalTrace,\n+      //    loader=Thread.currentThread.getContextClassLoader)\n+\n+      // RunnerUtils.testWithStsSched(schedulerConfig,\n+      //                  mcs,\n+      //                  trace,\n+      //                  Seq.empty,\n+      //                  violation,\n+      //                  stats,\n+      //       initializationRoutine=Some(runAndCleanup),\n+      //       preTest=Some(preTest), postTest=Some(postTest))\n+\n+      //RunnerUtils.printDeliveries(trace)\n+      */\n       RunnerUtils.runTheGamut(dir, mcs_dir, schedulerConfig, msgSerializer,\n         msgDeserializer, shouldRerunDDMin=shouldRerunDDMin,\n         populateActors=false,\n-        atomIndices=Some(Seq((1,8),(3,9),(21,22))),\n+        //atomIndices=Some(Seq((1,8),(3,9))), // (21,22)\n         loader=Thread.currentThread.getContextClassLoader,\n         initializationRoutine=Some(runAndCleanup),\n         preTest=Some(preTest), postTest=Some(postTest),",
      "status": "On branch spark-9256\nYour branch is ahead of 'origin/spark-9256' by 2 commits.\n  (use \"git push\" to publish your local commits)\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\n\n\tmodified:   interposition/src/main/scala/verification/schedulers/InteractiveScheduler.scala\n\tmodified:   invocation.txt\n\tmodified:   project/SparkBuild.scala\n\tmodified:   src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/DuplicateFileCrash.scala\n\tmodified:   src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")"
    }
  },
  "sys": {
    "lsb_release": "",
    "uname": "Darwin yossarian 14.5.0 Darwin Kernel Version 14.5.0: Tue Sep  1 21:23:09 PDT 2015; root:xnu-2782.50.1~1/RELEASE_X86_64 x86_64"
  },
  "timestamp": "2016_01_21_20_03_50",
  "user": "cs"
}
