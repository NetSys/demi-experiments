{
  "additional_metadata": null,
  "argv": [
    "./../../../../.././interposition/src/main/python/setup.py",
    "-t",
    "-n",
    "spark-fuzz"
  ],
  "cwd": "/Users/cs/Research/UCB/code/demi-applications",
  "host": {
    "cpu_info": "",
    "free": "",
    "name": "yossarian",
    "num_cores": "0",
    "uptime": "16:13  up  5:11, 6 users, load averages: 1.58 1.49 1.38"
  },
  "modules": {
    "sts2": {
      "branch": "spark-9256",
      "commit": "6d3b717184a0f375e94d14a45e7fa1c82143216b",
      "diff": "diff --git a/interposition/src/main/scala/verification/RunnerUtils.scala b/interposition/src/main/scala/verification/RunnerUtils.scala\nindex 1eb3f0e..6b7f220 100644\n--- a/interposition/src/main/scala/verification/RunnerUtils.scala\n+++ b/interposition/src/main/scala/verification/RunnerUtils.scala\n@@ -462,6 +462,7 @@ object RunnerUtils {\n \n   def deserializeMCS(experiment_dir: String,\n       messageDeserializer: MessageDeserializer,\n+      traceFile:String=ExperimentSerializer.minimizedInternalTrace,\n       scheduler: ExternalEventInjector[_] with Scheduler=null,\n       skipStats: Boolean=false, // if null, use dummy\n       loader:ClassLoader=ClassLoader.getSystemClassLoader()) :\n@@ -475,7 +476,8 @@ object RunnerUtils {\n     Instrumenter().scheduler = _scheduler\n     _scheduler.populateActorSystem(deserializer.get_actors)\n     val violation = deserializer.get_violation(messageDeserializer)\n-    val trace = deserializer.get_events(messageDeserializer, Instrumenter().actorSystem)\n+    val trace = deserializer.get_events(messageDeserializer,\n+      Instrumenter().actorSystem, traceFile=traceFile)\n     val mcs = deserializer.get_mcs\n     val actorNameProps = deserializer.get_actors\n     val stats = if (!skipStats) deserializer.get_stats else null\ndiff --git a/interposition/src/main/scala/verification/schedulers/InteractiveScheduler.scala b/interposition/src/main/scala/verification/schedulers/InteractiveScheduler.scala\nindex 26d9f03..889b3c9 100644\n--- a/interposition/src/main/scala/verification/schedulers/InteractiveScheduler.scala\n+++ b/interposition/src/main/scala/verification/schedulers/InteractiveScheduler.scala\n@@ -239,10 +239,42 @@ class InteractiveScheduler(val schedulerConfig: SchedulerConfig)\n   var externals : Seq[ExternalEvent] = null\n \n   def run(_externals:Seq[ExternalEvent]): Tuple2[EventTrace,Option[ViolationFingerprint]] = {\n+    /*\n+    preTestCallback()\n+\n+    // Kick off the system's initialization routine\n+    var initThread : Thread = null\n+    initializationRoutine match {\n+      case Some(f) =>\n+        logger.trace(\"Running initializationRoutine...\")\n+        initThread = new Thread(\n+          new Runnable { def run() = { f() } },\n+          \"initializationRoutine\")\n+        initThread.start\n+      case None =>\n+    }\n+    */\n+\n     externals = _externals\n     event_orchestrator.events.setOriginalExternalEvents(_externals)\n     Instrumenter().scheduler = this\n+\n+    /*\n+     wait for EndExternalAtomicBlocks?\n+     */\n+\n     return (execute_trace(_externals), violation)\n+\n+    /*\n+    postTestCallback()\n+\n+    // Wait until the initialization thread is done. Assumes that it\n+    // terminates!\n+    if (initThread != null) {\n+      logger.debug(\"Joining on initialization thread \" + Thread.currentThread.getName)\n+      initThread.join\n+    }\n+    */\n   }\n \n   // Record a mapping from actor names to actor refs\ndiff --git a/invocation.txt b/invocation.txt\nindex d082861..29187f9 100644\n--- a/invocation.txt\n+++ b/invocation.txt\n@@ -4,8 +4,7 @@\n /usr/bin/java -Xmx2500M -XX:MaxPermSize=256m -XX:+CMSClassUnloadingEnabled -jar /usr/local/Cellar/sbt/0.13.0/libexec/sbt-launch.jar \"project examples\" assembly\n \n # Then run\n-sbt \"project core\" \"run --master local-cluster[2,1,512] --class org.apache.spark.examples.STSSparkPi /Users/cs/Research/UCB/code/sts2-applications/src/main/scala/spark/examples/target/scala-2.10/spark-examples-1.1.0-SNAPSHOT-hadoop1.0.4.jar\"\n-\n+sbt \"project core\" \"run --master local-cluster[2,1,512] --class org.apache.spark.examples.STSSparkPi /Users/cs/Research/UCB/code/demi-applications/src/main/scala/spark/examples/target/scala-2.10/spark-examples-1.1.0-SNAPSHOT-hadoop1.0.4.jar\"\n \n -- Notes ---\n \ndiff --git a/project/SparkBuild.scala b/project/SparkBuild.scala\nindex 2b77e83..7d1d569 100644\n--- a/project/SparkBuild.scala\n+++ b/project/SparkBuild.scala\n@@ -73,7 +73,7 @@ object SparkBuild extends Build {\n     // For 2.10.*:\n     libraryDependencies += \"org.scala-lang\" % \"scala-swing\" % scalaVersion.value,\n     libraryDependencies += \"com.typesafe.akka\" %% \"akka-actor\" % \"2.2.3\",\n-    libraryDependencies += \"com.typesafe.akka\" %% \"akka-cluster\" % \"2.2.3\"\n+    libraryDependencies += \"com.typesafe.akka\" %% \"akka-cluster\" % \"2.2.3\",\n     libraryDependencies += \"org.scala-lang.modules\" % \"scala-jline\" % \"2.12.1\"\n \n     // -- /STS deps --\ndiff --git a/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala b/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala\nindex 7852483..59809aa 100644\n--- a/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala\n+++ b/src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala\n@@ -344,7 +344,7 @@ object STSSparkPi {\n       shouldShutdownActorSystem=false,\n       filterKnownAbsents=false,\n       ignoreTimers=false, // XXX\n-      invariant_check=Some(CrashUponRecovery.invariant))\n+      invariant_check=Some(DuplicateFileCrash.invariant))\n \n     val sched = new RandomScheduler(schedulerConfig,\n       invariant_check_interval=300, randomizationStrategy=new SrcDstFIFO)\n@@ -360,7 +360,7 @@ object STSSparkPi {\n       MyVariables.prematureStopSempahore.release()\n     }\n \n-    val fuzz = false\n+    val fuzz = true\n     if (fuzz) {\n       sched.nonBlockingExplore(externals, terminationCallback)\n \n@@ -384,14 +384,17 @@ object STSSparkPi {\n           sts.setPreTestCallback(preTest)\n           sts.setPostTestCallback(postTest)\n \n-          val dag = new UnmodifiedEventDag(initTrace.original_externals flatMap {\n+          val truncatedExternals = initTrace.original_externals flatMap {\n             case WaitQuiescence() => None\n             case WaitCondition(_) => None\n             case e => Some(e)\n-          })\n+          }\n+          val dag = new UnmodifiedEventDag(truncatedExternals)\n           // Conjoin the HardKill and the subsequent recover\n           atomicPairs foreach {\n             case ((e1, e2)) =>\n+              if ((truncatedExternals contains e1) &&\n+                  (truncatedExternals contains e2))\n               dag.conjoinAtoms(e1, e2)\n           }\n \n@@ -435,9 +438,9 @@ object STSSparkPi {\n \n     if (!fuzz) {\n       val dir =\n-      \"/Users/cs/Research/UCB/code/sts2-applications/experiments/spark-fuzz_2015_09_15_20_44_06\"\n+      \"/Users/cs/Research/UCB/code/demi-applications/experiments/spark-fuzz_2015_09_15_20_44_06\"\n       val mcs_dir =\n-      \"/Users/cs/Research/UCB/code/sts2-applications/experiments/spark-fuzz_2015_09_15_20_44_06_DDMin_STSSchedNoPeek\"\n+      \"/Users/cs/Research/UCB/code/demi-applications/experiments/spark-fuzz_2015_09_15_20_44_06_DDMin_STSSchedNoPeek\"\n \n       val msgSerializer = new BasicMessageSerializer\n       val msgDeserializer = new BasicMessageDeserializer(loader=Thread.currentThread.getContextClassLoader)\n@@ -445,6 +448,26 @@ object STSSparkPi {\n       def shouldRerunDDMin(externals: Seq[ExternalEvent]) =\n         false // XXX\n \n+      // val (trace, violation, _) = RunnerUtils.deserializeExperiment(mcs_dir,\n+      //   msgDeserializer,\n+      //   traceFile=ExperimentSerializer.minimizedInternalTrace,\n+      //   loader=Thread.currentThread.getContextClassLoader)\n+\n+      // val (mcs, trace, violation, actors, stats) = RunnerUtils.deserializeMCS(mcs_dir,\n+      //    msgDeserializer,\n+      //    traceFile=ExperimentSerializer.minimizedInternalTrace,\n+      //    loader=Thread.currentThread.getContextClassLoader)\n+\n+      // RunnerUtils.testWithStsSched(schedulerConfig,\n+      //                  mcs,\n+      //                  trace,\n+      //                  Seq.empty,\n+      //                  violation,\n+      //                  stats,\n+      //       initializationRoutine=Some(runAndCleanup),\n+      //       preTest=Some(preTest), postTest=Some(postTest))\n+\n+      //RunnerUtils.printDeliveries(trace)\n       RunnerUtils.runTheGamut(dir, mcs_dir, schedulerConfig, msgSerializer,\n         msgDeserializer, shouldRerunDDMin=shouldRerunDDMin,\n         populateActors=false,",
      "status": "On branch spark-9256\nYour branch is up-to-date with 'origin/spark-9256'.\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\n\n\tmodified:   interposition/src/main/scala/verification/RunnerUtils.scala\n\tmodified:   interposition/src/main/scala/verification/schedulers/InteractiveScheduler.scala\n\tmodified:   invocation.txt\n\tmodified:   project/SparkBuild.scala\n\tmodified:   src/main/scala/spark/examples/src/main/scala/org/apache/spark/examples/STSSparkPi.scala\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")"
    }
  },
  "sys": {
    "lsb_release": "",
    "uname": "Darwin yossarian 14.5.0 Darwin Kernel Version 14.5.0: Tue Sep  1 21:23:09 PDT 2015; root:xnu-2782.50.1~1/RELEASE_X86_64 x86_64"
  },
  "timestamp": "2016_01_21_16_13_22",
  "user": "cs"
}
